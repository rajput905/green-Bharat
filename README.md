# GreenFlow AI ğŸŒ¿

Real-time environmental intelligence system powered by Pathway, FastAPI, and OpenAI RAG.

---

## âœ¨ Features

| Layer | Technology |
|---|---|
| Real-time streaming | Pathway â€“ incremental computation |
| REST API | FastAPI + Uvicorn |
| AI / RAG | OpenAI GPT-4o + ChromaDB vector store |
| Data ingestion | JSONL files, Kafka, webhook push |
| Feature extraction | Keyword scoring, carbon relevance |
| Database | SQLAlchemy async (PostgreSQL / SQLite) |
| Frontend | Vanilla JS SSE dashboard (no build step) |
| Logging | Loguru rotating file + structured console |

---

## ğŸ“ Project Structure

```
greenflow/
â”œâ”€â”€ ingestion/          # Data source connectors (file, Kafka, webhook)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ingestor.py
â”œâ”€â”€ pipeline/           # Pathway streaming graph
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ streaming.py
â”œâ”€â”€ features/           # Feature extraction from raw records
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ extractor.py
â”œâ”€â”€ rag/                # Retrieval-Augmented Generation engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ engine.py
â”œâ”€â”€ api/                # FastAPI routers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ health.py   # GET /health, /ready
â”‚       â”œâ”€â”€ events.py   # POST/GET /events
â”‚       â”œâ”€â”€ query.py    # POST /query, /query/index
â”‚       â””â”€â”€ stream.py   # SSE + WebSocket
â”œâ”€â”€ database/           # SQLAlchemy models & session
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ session.py
â”œâ”€â”€ frontend/           # Static HTML/JS dashboard
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”œâ”€â”€ config.py           # Pydantic-Settings configuration
â”œâ”€â”€ main.py             # Application entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1 Â· Clone / enter project directory
```bash
cd "e:\green bharat hackthon\greenflow"
```

### 2 Â· Create virtual environment
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate
```

### 3 Â· Install dependencies
```bash
pip install -r requirements.txt
```

### 4 Â· Configure environment
```bash
copy .env.example .env      # Windows
# cp .env.example .env      # macOS / Linux
```

Open `.env` and fill in at minimum:
```env
OPENAI_API_KEY=sk-your-key-here
SECRET_KEY=some-long-random-string
```
For development, the rest of the defaults work out of the box (SQLite, local Chroma).

### 5 Â· Create required directories
```bash
mkdir -p data\watch\output logs
```

### 6 Â· Run the server
```bash
# Development (auto-reload)
uvicorn greenflow.main:app --reload --host 0.0.0.0 --port 8000

# Or via Python
python greenflow/main.py
```

Open your browser at **http://localhost:8000**

---

## ğŸ“¡ API Endpoints

| Method | Path | Description |
|---|---|---|
| GET | `/api/v1/health` | Liveness check |
| GET | `/api/v1/ready` | Readiness check |
| POST | `/api/v1/events` | Ingest a green-data event |
| GET | `/api/v1/events` | List recent events |
| POST | `/api/v1/query` | Ask AI (RAG pipeline) |
| POST | `/api/v1/query/index` | Index a document |
| GET | `/api/v1/stream/events` | SSE live feed |
| WS | `/api/v1/stream/ws` | WebSocket connection |

> Interactive docs at **http://localhost:8000/docs** (development mode only).

---

## ğŸ” Pathway Streaming Pipeline

The pipeline is in `pipeline/streaming.py`. Enable it in `main.py` by uncommenting:

```python
from pipeline.streaming import run_pipeline
t = run_pipeline()
```

Drop JSONL files into `data/watch/` and Pathway will:

1. Detect them automatically (no restart needed)
2. Apply UDFs: decode payload, classify source, compute carbon score
3. Write enriched rows to `data/watch/output/enriched.jsonl`
4. The SSE endpoint tails that file and pushes to the browser in real time

**Example event file** `data/watch/sample.jsonl`:
```json
{"source": "sensor_42", "timestamp": 1700000000.0, "payload": "{\"text\": \"CO2 levels rising near urban zone\", \"co2_ppm\": 425.3}"}
```

---

## ğŸ¤– RAG Usage

```python
from rag.engine import rag_engine

# Index a document
rag_engine.index_document("Solar irradiance dropped 12% in Q3 2024", metadata={"region": "north"})

# Query
import asyncio
result = asyncio.run(rag_engine.query("What happened to solar irradiance?"))
print(result["answer"])
```

---

## ğŸ³ Docker Deployment

The fastest way to get the full stack (Database, Engine, API, Dashboard) running is via Docker Compose.

### 1 Â· Configure Environment
Ensure your `.env` file has the necessary keys:
```env
OPENAI_API_KEY=sk-your-key-here
WEATHER_API_KEY=your-key       # Optional
AQI_API_KEY=your-key           # Optional
```

### 2 Â· Spin up the stack
```bash
docker-compose up --build -d
```

This will launch:
- **Greenflow DB**: PostgreSQL at port 5432
- **Greenflow Engine**: Pathway processing pipeline
- **Greenflow API**: FastAPI backend at port 8000
- **Greenflow UI**: Streamlit dashboard at port 8501

### 3 Â· Check logs
```bash
docker-compose logs -f api-backend
```

---

## ğŸ§ª Run Tests

```bash
pytest tests/ -v
```

---

## ğŸŒ Environment Variables Reference

See `.env.example` for the full list with inline documentation.

| Variable | Default | Required |
|---|---|---|
| `OPENAI_API_KEY` | â€” | âœ… |
| `SECRET_KEY` | â€” | âœ… |
| `DATABASE_URL` | SQLite dev | âŒ |
| `PATHWAY_LICENSE_KEY` | (open tier) | âŒ |
| `KAFKA_BROKER` | localhost:9092 | âŒ |

---

## ğŸ›  Production Deployment

```bash
# 4 Uvicorn workers
uvicorn greenflow.main:app --workers 4 --host 0.0.0.0 --port 8000
```

Set in `.env`:
```env
APP_ENV=production
APP_DEBUG=false
DATABASE_URL=postgresql+asyncpg://user:pass@host:5432/greenflow
```

---

## ğŸ“ License

MIT Â© 2025 GreenFlow AI Team
